{
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "pandas_final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pandas\n",
        "\n",
        "pandas is a software package written for the Python programming language for data manipulation and analysis. In particular, it offers data structures and operations for manipulating tabular data and time series. \n"
      ],
      "metadata": {
        "id": "5fCEDCU_qrC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning objective\n",
        "\n",
        "We will use an example from a real research project and use pandas to walk through steps that social scientists usually take to handle raw data.\n"
      ],
      "metadata": {
        "id": "rMExucF44s_c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Import pandas\n",
        "\n",
        "To begin, let's look at a few ways to import pandas"
      ],
      "metadata": {
        "id": "GJBs_flRovLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import specific classes and functions: useful if you know you'll only be needing the most common parts of pandas\n",
        "from pandas import DataFrame, read_csv\n",
        "\n",
        "# General syntax to import a package but no functions: \n",
        "#   import (package) as (give the package a nickname/alias)\n",
        "# Most pandas users will import the entire package and give it the short nickname \"pd\" as follows:\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "gJr_9dXGpJ05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data example\n",
        "\n",
        "We are going to use an example dataset retrieved from Stack Overflow.\n",
        "\n",
        "Stack Overflow is a question and answer website. Users can ask and answer questions related to programming. Here is an example question and all the answers:\n",
        "\n",
        "https://stackoverflow.com/questions/4/how-to-convert-a-decimal-to-a-double-in-c\n",
        "\n",
        "Stack Overflow data is publicly available, which allows researchers to observe user behavior in the community.\n",
        "\n",
        "Here is an overview of the data schema:\n",
        "\n",
        "https://meta.stackexchange.com/questions/2677/database-schema-documentation-for-the-public-data-dump-and-sede\n",
        "\n",
        "Stack Overflow data is commonly used in social science research, especially studies about expertise and knowledge sharing! Here are just a few examples:\n",
        "- [How do programmers ask and answer questions on the web?](https://dl.acm.org/doi/abs/10.1145/1985793.1985907)\n",
        "- [Discovering Value from Community Activity on Focused Question Answering Sites: A Case Study of Stack Overflow](https://dl.acm.org/doi/abs/10.1145/2339530.2339665)"
      ],
      "metadata": {
        "id": "iUTpAqJ6ykG4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Import csv data\n",
        "\n",
        "As mentioned in the introduction, pandas is designed to work with tabular data. Tabluar data is most commonly provided to researchers in a simple spreadsheet format known as csv, which stands for *comma separated values*. Because of this, pandas has a function to load tabular data from a csv file:  `read_csv`. Let us take a look at this function and what inputs it takes."
      ],
      "metadata": {
        "id": "9c2d00fn3zDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "help(read_csv)"
      ],
      "metadata": {
        "id": "0Qfk96MX10T0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Common parameters in **read_csv**\n",
        "\n",
        "read_csv(filepath, sep= , header= , usecols=None)\n",
        "\n",
        "- filepath_or_buffer : str, path object or file-like object\n",
        "   \n",
        "- sep : str, default ','\n",
        "    \n",
        "- header : int, list of int\n",
        " - Row number(s) to use as the column names, and the start of the data.  \n",
        "    \n",
        "- names : array-like, optional\n",
        " - List of column names to use, if the file contains a header row\n",
        "   \n",
        "- usecols : list-like or callable\n",
        " - Return a subset of the columns. \n",
        " - For example, a valid list-like\n",
        "    `usecols` parameter would be ``[0, 1, 2]`` or ``['foo', 'bar', 'baz']``.\n",
        "   \n"
      ],
      "metadata": {
        "id": "OL5f9X4l1XLa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see from the parameters description, we need to give `read_csv` a path to the csv file we want to load. Here, we will load the example StackOverflow data, which is contained in the file \"so_question.csv\" in our GitHub repo."
      ],
      "metadata": {
        "id": "8N8zDRbDjl6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The following path is valid for our Binder setup. If you are running this notebook in your local machine instead and have the file downloaded to a different location, you may need to change this path.\n",
        "Location = './data/so_question.csv'\n",
        "df = pd.read_csv(Location)"
      ],
      "metadata": {
        "id": "7X7PRKhX4kJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`read_csv` loads the data into a structure known as a **DataFrame**, as we can see if we check the type of the variable `df` that we created:"
      ],
      "metadata": {
        "id": "oRsvGySgjl6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type(df)"
      ],
      "metadata": {
        "id": "8ZmhPaCsjl6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Think of a **DataFrame** as a table or spreadsheet that you can work with inside your Python code! Like any table, it contains data organized into rows and columns.\n",
        "\n",
        "Now you might be wondering what the data actually looks like. The best way to find out is to use the `display` function, which will show a sample of data from the DataFrame. This is always the first thing you should do after loading a new dataset in pandas!"
      ],
      "metadata": {
        "id": "cE0uTGw6jl6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(df)"
      ],
      "metadata": {
        "id": "NGGMN_Ti28h3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This brings us to the first problem of the exercise. The `read_csv` function treated the first record in the csv file as the header names. This is obviously not correct since the text file did not provide us with header names; instead, we can see that the first row looks like real data.\n",
        "\n",
        "To correct this we will pass the `header` parameter to the read_csv function and set it to **None** (means null in python)."
      ],
      "metadata": {
        "id": "in0J3EXx3eJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(Location, header=None)\n",
        "display(df)"
      ],
      "metadata": {
        "id": "OxDMJ17V3hW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that all 10000 rows of the csv file are now correctly treated as real data. Since no header names were given, pandas automatically just numbers the headers starting from 0. This is obviously not very useful, usually you want your columns to have meaningful names describing what kind of data they contain. In this case, the documentation for the StackOverflow dataset tells us what the columns are: \n",
        "- The first column is an ID\n",
        "- The second column is a timestamp\n",
        "- The third column is a topic tag\n",
        "- The fourth column is a year\n",
        "- The fifth column is a month\n",
        "- The sixth column is the ID of the accepted answer, if any\n",
        "- The seventh column is the ID of the author\n",
        "\n",
        "Now we want to tell pandas to use column names that are descriptive of this data. We can do this by passing a list of column names to the parameter `names`. This also lets us omit the `header` parameter."
      ],
      "metadata": {
        "id": "L2wafQlD3_S5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(Location, names=['id', 'creation_time', 'tag', 'year','month', 'accepted_id','owner_user_id'])\n",
        "display(df)"
      ],
      "metadata": {
        "id": "7d3S5YmR4ERa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice the leftmost column is special: it does not have a name and simply contains consecutive numbers [0,1,2,3,4,...]. This is the *index* of the DataFrame; you can think of these as the row numbers in an Excel file. This index is also similar to the primary key of a sql table with the exception that an index is allowed to have duplicates.\n",
        "\n",
        "One last thing we want to point out before we move on: notice that in the accepted_id column, there seems to be a mix of data: some look like numerical IDs, but others say \"NaN\". NaN is short for \"Not A Number\" and is pandas' way of representing *missing data*. Think of NaNs as equivalent to an empty cell in an Excel file. The meaning of missing data depends on the context of the dataset. In this case, since this data is from Stack Overflow and the \"accepted_id\" column represents the ID of an accepted answer, we can infer that in this case missing data means that the question never received an accepted answer. In research, we often want to do something special to handle missing data; we will return to this later in the workshop."
      ],
      "metadata": {
        "id": "iwv6zshx5ylG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Checking the structure of the data\n",
        "Usually we first do a couple of sanity checks to make sure the data we imported is in good shape and understand its high level structure. \n",
        "\n",
        "Usually we make sure the data type of each variable is sensible, check the total number of observations, and the number of variables in the data set.\n",
        "\n",
        "Let's take a look at the example data set. First, we'll check data types using the `dtypes` variable.\n"
      ],
      "metadata": {
        "id": "OwuxHmxllTwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check data type of the columns\n",
        "df.dtypes"
      ],
      "metadata": {
        "id": "yCC64Uqy6nAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check data type of a specific column\n",
        "df.accepted_id.dtype"
      ],
      "metadata": {
        "id": "OI7fDnZh6tZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we'll check the number of observations (rows) in the dataset. This can be done by examining the length of the DataFrame's *index*:"
      ],
      "metadata": {
        "id": "NmbZVWO2BSLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(df.index)"
      ],
      "metadata": {
        "id": "QIoJ_maABVOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, we can check the number of variables (columns) in the dataset by examining the length of the DataFrame's *columns* list:"
      ],
      "metadata": {
        "id": "IWQpJOKXBkCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(df.columns)"
      ],
      "metadata": {
        "id": "iwu71C2hBmVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Slicing and Indexing\n",
        "Usually, we will not be working with an entire data set all at the same time. Instead, we usually want to pick out specific rows or columns to analyze and work with. In pandas, this can be done using *slicing and indexing*."
      ],
      "metadata": {
        "id": "g0z8mSwz6ZvH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most basic indexing operation is selecting a specific column. This can be done using standard Python indexing syntax (square brackets) and giving it the name of the column you want. For example, the following code will select the \"id\" column:"
      ],
      "metadata": {
        "id": "SwTB_xAcjl6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id_column = df['id']\n",
        "display(id_column)"
      ],
      "metadata": {
        "id": "Qr7i1ojWjl6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The syntax for selecting a row is similar, except that you need to tell pandas that you are trying to access a row, since indexing defaults to columns. To do this, use the `loc` variable:"
      ],
      "metadata": {
        "id": "bNOAR5qfjl6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fifth_row = df.loc[4] # remember the index starts from 0, so the fifth row is at index 4\n",
        "display(fifth_row)"
      ],
      "metadata": {
        "id": "w0IFERlnjl6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both row indexing and column indexing allow you to select multiple rows or columns at once. To do this, you can provide a **list** of row indices or column names, instead of just one:"
      ],
      "metadata": {
        "id": "hFNKSwHOjl6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "multi_columns = df[['id', 'creation_time']]\n",
        "display(multi_columns)"
      ],
      "metadata": {
        "id": "WvDmj7y6HwJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multi_rows = df.loc[[0,1,2,3]]\n",
        "display(multi_rows)"
      ],
      "metadata": {
        "id": "m_aQBFI3jl6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since row indices are numerical, python's **slicing** syntax can also be used on them to select a range of rows. This can simplify your code a lot because you can specify a range rather than writing out full list of indices! For example, the following code is equivalent to the previous code:"
      ],
      "metadata": {
        "id": "yAraQcG2jl6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "multi_rows = df.loc[0:3] # important difference from regular python: pandas slices include *both* indices!\n",
        "display(multi_rows)"
      ],
      "metadata": {
        "id": "y92xDXcYjl6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additionally, we may sometimes want to pick out a **single** value from the DataFrame. For example, suppose we want to find the question ID in the fifth row. We can do this by giving both a row and column to the `loc` indexer. Note that the row and column **must** be given in that order (you cannot give the column first) and should be separated by a comma, as shown below:"
      ],
      "metadata": {
        "id": "2NYv4_YSKO4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fifth_id = df.loc[4,\"id\"]\n",
        "print(fifth_id)"
      ],
      "metadata": {
        "id": "iSrJcPshKg1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1\n",
        "Finally, the same syntax can be used to select multiple rows **and** columns at the same time; you simply need to provide a list or range of rows, followed by a list of column names, separated by a comma.\n",
        "\n",
        "Let's try get first 4 rows, for column 'id' and 'creation_time'"
      ],
      "metadata": {
        "id": "xiAc_0QGjl6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "multi_row_and_col = df.loc[____,[_____]]\n",
        "display(multi_row_and_col)"
      ],
      "metadata": {
        "id": "4Y3Ywvp8jl6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2\n",
        "\n",
        "What is the tag of the 50th question in the data set?"
      ],
      "metadata": {
        "id": "CqhRO0k3BvM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.loc[49,\"tag\"])"
      ],
      "metadata": {
        "id": "qAnp6yhjCL68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Summarizing column data\n",
        "Previously, we sanity checked our data by checking its structure (data types and number of rows and columns). A second common step in preliminary exploration of the data is to examine individual columns (with the help of indexing, as covered previously). pandas can help with this process by offering several methods to *summarize* the data in a column."
      ],
      "metadata": {
        "id": "6lBVwk2sjl6Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summarizing numerical data\n",
        "Some columns contain numerical data. For example, years in the \"year\" column are represented as numbers. For numerical data, pandas offers a number of methods that implement common mathematical summary functions, such as finding the min and the max:"
      ],
      "metadata": {
        "id": "MdRkt4Prjl6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[\"year\"].min()) # what's the earliest year in the data?\n",
        "print(df[\"year\"].max()) # what's the latest year in the data?"
      ],
      "metadata": {
        "id": "M__x7ovSjl6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summarizing categorical data\n",
        "Mathematical operations like min and max are useful for quickly summarizing numerical data. But not all columns are numerical. For example, the \"tag\" column contains text. But notice that the text in the \"text\" column is not just any arbitrary text! Rather, it seems to take on specific, repeated values, like \".htaccess\" or \"zxing\". Therefore, \"tag\" is what data scientists refer to as a *categorical variable*: one that can only take on values from a fixed, finite set of possibilities. To summarize categorical data, researchers generally want to know what is the set of values the variable can take? In pandas, this can be done using the `unique` method:"
      ],
      "metadata": {
        "id": "_JT9EoIUjl6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[\"tag\"].unique())"
      ],
      "metadata": {
        "id": "Nib9a0S_jl6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2\n",
        "\n",
        "- Which years and months of questions are in the data?\n"
      ],
      "metadata": {
        "id": "jRw50ceO_b_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(____________) # fill in your code for years here\n",
        "print(____________) # fill in your code for months here"
      ],
      "metadata": {
        "id": "IsL_ZU9BAiSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Querying using conditions based on column values\n",
        "\n",
        "Now we can start looking at more advanced data analysis. Previously, we selected subsets of rows by giving the `loc` indexer a list or range of row indices. Most often, however, we don't know ahead of time which rows we want to analyze; instead, we more often want to select rows based on some conditions. For example, we might want all questions that took place in a specific year, or all questions that have a specific tag. To achieve this, the `loc` indexer can actually be used with conditions, not just with indices! \n",
        "\n",
        "This is best explained through examples. Let's start with the first example question, looking for all questions that took place in a specific year, let's say 2008. The syntax for the comparison would be `df['year'] == 2008`, and we can pass this comparison directly to `loc`, as follows:"
      ],
      "metadata": {
        "id": "vIQI7zsSC2FF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "year2008 = df.loc[df['year'] == 2008]\n",
        "display(year2008)"
      ],
      "metadata": {
        "id": "IHWjCNENBH43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that this syntax is similar to the filtering syntax used by numpy (as seen in the previous Python workshop) and by R. Alternatively, if you are familiar with SQL, you can think of the above code as being equivalent to the SQL query `select * from df where year=2008`.\n",
        "\n",
        "Like in numpy, you can include multiple comparisons in a single query, combining them using operators `&` for \"and\" and `|` for \"or\". So, for example, if we wanted only questions from August 2008:"
      ],
      "metadata": {
        "id": "pW5ot28Fjl6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "august2008 = df.loc[(df['year']==2008)&(df['month']==8)]\n",
        "display(august2008)"
      ],
      "metadata": {
        "id": "B4xk3-AJDAPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that when we run a query, the matching rows get returned in a new DataFrame, which is a subset of the original, containing only rows that matched the query. We can confirm that the result is still a DataFrame by checking the type of the variable:"
      ],
      "metadata": {
        "id": "6QlU0jFUjl6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type(august2008)"
      ],
      "metadata": {
        "id": "hG979xWhjl6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This means that we can run all of the previously described DataFrame methods on the query results! For example, this can be useful for finding out how many rows matched the query:"
      ],
      "metadata": {
        "id": "OppCv7_Ojl6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(august2008.index)) # we can use the same code we saw earlier for counting rows, because august2008 is still a DataFrame"
      ],
      "metadata": {
        "id": "RuZIkruEjl6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3\n",
        "\n",
        "How many questions in 2013 had the tag \"python\"?"
      ],
      "metadata": {
        "id": "90HyVcgYjl6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "python2013 = df.loc[______________] # first, fill in your query here\n",
        "print(_____________) # then, fill in the code to count the number of rows"
      ],
      "metadata": {
        "id": "X-DlbQVWjl6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Modifying and adding rows\n",
        "Queries aren't just useful for accessing data; we may sometimes also want to *modify* data that matches a query! One common reason to do this is for data cleaning. For example, some questions in our data set have the tag \".net\" and others have the tag \"net\", but both of these actually refer to the same thing (the Microsoft dotNET framework). So when we clean the data, we might want to standardize all instances of \".net\" to \"net\". This can be done with the following steps:\n",
        "\n",
        "1. Write a query to find all instances of the thing you want to clean (in this case, questions with the tag \".net\") (See Section 7)\n",
        "2. Use row-and-column indexing to combine this query (which selects certain rows) with column selection (See Section 5)\n",
        "3. Use Python assigment syntax (the \"=\" operator) to set the new value\n",
        "\n",
        "Again, this is best understood through an example. The following code implements all the steps to standardize \".net\" to \"net\":"
      ],
      "metadata": {
        "id": "yZ-cXW6P6Opk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert .net to net\n",
        "df.loc[(df['tag'] == '.net'), 'tag'] = 'net'\n",
        "# to confirm that it worked, count how many questions are now tagged \"net\" and how many are tagged \".net\" (the latter should be zero)\n",
        "print(len(df.loc[df['tag'] == 'net'].index))\n",
        "print(len(df.loc[df['tag'] == '.net'].index))"
      ],
      "metadata": {
        "id": "EVP5mQ1R6Tl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we *replaced* values in an existing column. But there are other cases where instead, you might want to preserve the original data, and instead put the cleaned data in a new column. pandas lets us do this too! As an example, let's recall the missing values in the \"accepted_id\" column that we told you about earlier. Remember that the interpretation of these missing values is that the question did not receive an accepted answer. For some research questions, we might simply want to know *whether* a question received an accepted answer or not, without caring about the ID of the answer. But this does not mean we want to completely replace the \"accepted_id\" column, because for *other* research questions we might still care about the answer IDs! So instead, we might want to create a *new* column that contains the simplified information of whether a question received an accepted answer.\n",
        "\n",
        "We will start by creating a new column, let's call it \"has_answer\". Initially, we'll create the column as a copy of \"accepted_id\"; we can then modify it (using code similar to what we did previously for \"tag\") without affecting the original column. To create a new column, we can simply use Python assigment syntax:"
      ],
      "metadata": {
        "id": "G5lS8Sf8jl6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"has_answer\"] = df[\"accepted_id\"]\n",
        "display(df)"
      ],
      "metadata": {
        "id": "hNWb6wQAjl6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that there is now a new column \"has_answer\" that is an exact copy of \"accepted_id\". Next we will modify it as follows: rows containing missing values will be set to 0 (representing \"no answer\") and the rest will be set to \"1\" (representing \"has answer\").\n",
        "\n",
        "Syntax notes: to check if a value is missing, pandas provides the special comparison function `isna`. To negate a comparison (so we can say when a row does **not** contain a missing value), we'll use the the `~` operator."
      ],
      "metadata": {
        "id": "oIMcZ7Cbjl6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[~(pd.isna(df['has_answer'])), 'has_answer'] = 1\n",
        "df.loc[(pd.isna(df['has_answer'])), 'has_answer'] = 0\n",
        "display(df)"
      ],
      "metadata": {
        "id": "oj2Fb7hhjl6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Fine-grained analysis using groupby\n",
        "In section 6, we showed how to summarize data from an *entire* column, such as taking the min and max of numerical data. This is useful for initial exploration, but when tackling actual research questions, we often want to do more complicated operations involving interactions between multiple variables. For example, instead of just finding the earliest year in the data, we might be interested in finding the earliest year each tag first appeared. Using what we have learned so far, one way to do this would be to use queries: for each tag, you could write a query to select only the rows containing that tag, and call the `min` method each time. But this is extremely tedious, given that there are so many tags in the data! Thankfully, pandas offers a faster alternative: `groupby`.\n",
        "\n",
        "The `groupby` method can be thought of a splitting a DataFrame based on some categorical variable. For example, if we use `groupby` on the \"tag\" column, we'll have one group containing rows tagged \".htaccess\", another containing rows tagged \"python\", and so on, for every unique tag. Now, if we just run `groupby` on its own, we won't immediately see anything useful:"
      ],
      "metadata": {
        "id": "TwLU5fCWjl6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_grouped = df.groupby(\"tag\")\n",
        "display(df_grouped) # prints out some strange code that isn't super useful..."
      ],
      "metadata": {
        "id": "D0LNym1Pjl6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "But the key difference happens when we call summarization methods on the grouped variable. Instead of running the summarization on the entire data, as happens normally, the summarization will be run separately on each group! Let's try using the `min` summarization method on \"year\" in the grouped data:"
      ],
      "metadata": {
        "id": "BG_OSJQ3jl6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(df_grouped['year'].min()) # we can use the same indexing and summarization syntax as we did for regular DataFrames, but the operation now happens separately for each group!"
      ],
      "metadata": {
        "id": "4qk2lYsKjl6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, instead of a single minimum, we get multiple: one for each group! Specifically, we are seeing the earliest year each tag first appeared. We can immediately see how this might be useful for tracking trends; for example, questions tagged \".htaccess\" started in 2010, while questions tagged \".htpasswd\" didn't start until 2019. This might suggest, for example, that .htpasswd is a newer tag."
      ],
      "metadata": {
        "id": "6zx0vHUmjl6W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are other kinds of summarization methods that can be useful when combined with `groupby`. Here are just a few examples:"
      ],
      "metadata": {
        "id": "-QL_oq4Cjl6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The count() method counts the total number of unique items in a column.\n",
        "# When combined with groupby, the counting will happen separately per group.\n",
        "# Let's try using it to find out how many questions happened per year (by counting the number of unique question IDs per year group)\n",
        "df_grouped = df.groupby(\"year\")\n",
        "display(df_grouped[\"id\"].count())"
      ],
      "metadata": {
        "id": "D6A5wTMQjl6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The sum() method computes the sum of all items in a column\n",
        "# When combined with groupby, sums are computed separately per group\n",
        "# Let's try using it to find out how many questions were answered per year.\n",
        "df_grouped = df.groupby(\"year\")\n",
        "display(df_grouped[\"has_answer\"].sum())"
      ],
      "metadata": {
        "id": "omhvRRokjl6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, note that it is possible to group on multiple columns at once! This will create a group for each unique pair of possible values in the two columns. For example, we might be interested not just in years, but in specific year-month combinations:"
      ],
      "metadata": {
        "id": "lKEE8WO0jl6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_grouped = df.groupby([\"year\", \"month\"])\n",
        "display(df_grouped[\"id\"].count())"
      ],
      "metadata": {
        "id": "gjbzKU5Ejl6W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}